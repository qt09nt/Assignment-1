---
  title: "Assignment 1"
author: "Crystal Zhu"
date: "1/28/2021"
output:
  html_document: default
pdf_document: default
---
  ## Check the dimension of the dataset
  
  ```{r, echo=FALSE}
library(openxlsx)
X=read.xlsx("adultdata.xlsx")
summary(X)
dim(X)
#32561 records in "training data" (2/3 of total 48842 records)
#16281 records in the "test data", which is saved in another Excel file
```


## Check duplicates

```{r, echo=FALSE}
library(tidyverse)
#Remove duplicated rows based on all columns
# - if two rows are exactly the same, keep only one of them
X_nodup=distinct(X,X[,1:15], keep_all=TRUE)[,-16]

#identify duplicated rows
X_dup=X[duplicated(X),]
dim(X_dup)
#the row_id in X_dup is the row index in the original dataset X
#There are 24 duplicated rows that have same values for all variables, so remove them from analysis.

#use X_nodup for all later analysis
X=X_nodup
```

## <span style="color: red;"> Recode the target variable to 0/1 or Y/N? </span>

## Check missing values.
```{r , echo=FALSE}
#echo=FALSE only stops the code from printing out, not the results


#check missing values of each column
m=c()
for (i in 1:ncol(X))  {
  m[i]=sum(grepl("?",X[,i],fixed = TRUE))
}
missval=paste0(colnames(X),rep("-",15),m,rep(" missing values",))
missval
```
From the above, there are missing values in the data.

## Check data types

```{r, echo=FALSE}

datatype=sapply(X,class)
datatype
#6 numeric variables
#9 categorical variables
```

We can see there are both numeric and categorical variables in the dataset.

## Check outliers for numeric variables
```{r, echo=FALSE}
par(mar = c(1,1,1,1))
par(mfrow=c(2,3))
for (i in 1:ncol(X)){
  
  if (class(X[,i])=="numeric") {
    boxplot(X[,i], main=colnames(X)[i])
  }
}

#Since there are large number of 0s in capitalgain & capitalloss variables, check outliers for non-zero values
par(mfrow=c(1,2))
boxplot(X$capitalgain[which(X$capitalgain!=0)],main="Outliers for non-zero Capitalgain")

boxplot(X$capitalloss[which(X$capitalloss!=0)],main="Outliers for non-zero Capitalloss")

#There are still outliers even excluding zeros for capitalgain and capitalloss variables.
```


There are many outliers for all numeric variables.

### <span style="color:red;"> Deal with outliers! </span>

## Check validality of column values

```{r, echo=FALSE}
for (i in 1:ncol(X)) {
  if (class(X[,i])=="numeric") {
    cat("[",i,"]", colnames(X)[i], "Numeric","Min Mean Max: ",summary(X[,i])[c(1,4,6)],"\n")
  } else {
    cat("[",i,"]",colnames(X)[i],"Categorical", unique(X[,i]),"\n")
  }
  
}


```

## <span style="color:red;"> Recode the country variable where it has values "South" & "Hong" </span> - 20 records are from Hong Kong, so rename Hong to Hong Kong


# Exploratory data analysis

## Check redundancy and correlations among variables - how one attribute's values vary from those of another

### pairs plot for numeric variables

```{r, echo=FALSE}
#pairs plot for numeric variables
numindex=datatype=="numeric"
pairs(scale(X[,numindex]))

#pairs plot may work better on standardized numeric values?
#what's with capitalgain & capitalloss?
```

### correlations

Pearson's correlation for numeric variables

```{r, echo=FALSE, message=FALSE}
library("Hmisc")
cormat <- rcorr(as.matrix(X[,numindex]))
cormat

#Draw a correlogram
library(corrplot)
corrplot(cormat$r, type = "upper", 
         tl.col = "black", tl.srt = 45)
#for more info http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software 

#Draw a chart of a correlation matrix
library(PerformanceAnalytics)
chart.Correlation(scale(X[,numindex]), histogram=TRUE, pch=19)

```

## Chi-square test & Cramer's V to show associations between categorical variables

```{r, echo=FALSE,message=FALSE}
#If many of the expected counts are very small, the Chi-squared approximation may be poor

X_cat=subset(X,select=c(datatype=="character"))

#all combinations from 1-9
#expand.grid will create dups (c(2,1) & c(1,2)), so don't use it
allcom=combn(ncol(X_cat),2)
#allcom is 2*36, each column is a combination of 1-9
#the first row is the index for var1, the second row is the index for var2
teststat=c()
pvalue=c()
ind1=c()
ind2=c()
cramv=c()
chisqmat=matrix(,9,9)
pmat=matrix(,9,9)
crammat=matrix(,9,9)

library(DescTools)

#use suppressWarnings() to suppress showing the warning msgs from chisq.test

suppressWarnings (
  for (i in 1:ncol(allcom)) {
    
    teststat[i]=chisq.test(X[,allcom[,i][1]],X[,allcom[,i][2]])$statistic
    pvalue[i]=chisq.test(X[,allcom[,i][1]],X[,allcom[,i][2]])$p.value
    ind1[i]=allcom[,i][1]
    ind2[i]=allcom[,i][2]
    cramv[i]=CramerV(X[,allcom[,i][1]],X[,allcom[,i][2]])
    chisqmat[allcom[,i][1],allcom[,i][2]]=teststat[i]
    pmat[allcom[,i][1],allcom[,i][2]]=pvalue[i]
    crammat[allcom[,i][1],allcom[,i][2]]=cramv[i]
    
  })

sum(pvalue<0.05)
#all pvalues are less than 0.05 - all categorical variables are significantly associated with each other
colnames(chisqmat)=colnames(X_cat)
rownames(chisqmat)=colnames(X_cat)
colnames(pmat)=colnames(X_cat)
rownames(pmat)=colnames(X_cat)
colnames(crammat)=colnames(X_cat)
rownames(crammat)=colnames(X_cat)
chisqmat
pmat
crammat

```




## Barcharts for categorical variables

```{r, echo=FALSE}
library(ggplot2)
library(ggpubr)
par(mar = c(2,2,2,2))
par(mfrow=c(3,3))
g= vector('list', 9)
for (i in 1:ncol(X)) {
  
  if (class(X[,i])=="character") {
    
    g[[i]]=print(ggplot(data=X, aes(x=X[,i])) + facet_grid(.~X[,15]) + geom_bar(fill="red") + xlab(colnames(X)[i]))       
    
  }
  
}

#ggarrange(g[[1]],g[[2]],g[[3]],g[[4]],g[[5]],g[[6]],g[[7]],g[[8]],g[[9]],ncol #= 3, nrow = 3)
```


### <span style="color:red;">  Concept hierarchy generation for some variables? </span>
### <span style="color:red;"> Discretization (e.g. numeric age into [15,30] [31, 50]...) - maybe use these categorical groups together with the original numeric variable when use tree based method, which allows for correlated features? </span>

### Imbalanced dataset  - oversampling 

### encoding categorical variables?

## barcharts/scatter plots by target variable

```{r}


```

### <span style="color: red;"> maybe train several models first without removing outliers or feature engineering, then train another several models with feature engineering (e.g. combine some categorical groups, impute missing, outlier - a separate model/replace with group mean?, create new features from existing ones?) - then compare accuracy and intepreterbility etc between models </span>

### <span style="color: red;"> Model evaluation - do not overfit. </span>

### <span style="color: red;">  Perform supervised learning using several methods for different features. </span>

### <span style="color: red;"> Check assumptions of supervised learning methods to see whether the data meets. </span>

### <span style="color: red;"> Then use the best model among all to deploy ShinyApp

## <span stype="color: red;"> Use internal and external validation measures to describe and compare the models and the predictions (some visual methods would be good). </span>


## Train supervised learning models
```{r}
library(caret)
library(tidyverse)

#train-test split (a 80-20 split)
set.seed(300)
X_index=createDataPartition(X$income, p=0.8, list=FALSE)
X_train=X[X_index,]
X_test=X[-X_index,]

#default training method for train() is random forest (rf)
#default is no pre-process
#default resampling is bootstrap. To change to CV, use the trainControl() function.

#define resampling
#default method is boot, number is the #of fold is method is cv.
cv_5=trainControl(method = "cv", number = 5, allowParallel = TRUE )

#tune models
#can't run rf - run out of rstudio cloud memory
#knn (78.8%,k=9), glm (85%), gbm-Stochastic Gradient Boosting Tree (86.3%) works
#LMT - Logistic Model Trees	
#xgbTree - eXtreme Gradient Boosting
#adaboost - AdaBoost Classification Trees	
#lssvmLinear - Least Squares Support Vector Machine	
#nnet - Neural Network
#qrnn - Quantile Regression Neural Network	
#default evaluation metric is accuracy
#no pre-processing
model_knn=train(income ~.,data = X_train, trControl=cv_5, method="knn" )
model_glm=train(income ~.,data = X_train, trControl=cv_5, method="glm")
model_gbm=train(income ~.,data = X_train, trControl=cv_5, method="gbm")
model_xgbTree=train(income~.,data = X_train, trControl=cv_5, method="xgbTree")
model_adaboost=train(income~., data = X_train, trControl=cv_5, method="adaboost")
model_lssvmLinear=train(income~.,data = X_train, trControl=cv_5,method="lssvmLinear")
model_rpart=train(income~.,data = X_train, trControl=cv_5,method="rpart")

#https://stats.stackexchange.com/questions/53240/practical-questions-on-tuning-random-forests
#https://stackoverflow.com/questions/23075506/how-to-improve-randomforest-performance
#https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769

#model results
model_knn$results
model_glm$results
model_gbm$results
...

#Tuning parameters
model_knn$bestTune
model_glm$bestTune
...

#the final model (the model fit on the best tuning parameter on the training data?)
model_knn$finalModel

#predictions based on the training model
#predicted classes for each record in the testing data
pred_knn=predict(model_knn,X_test)
#predicted probabilities for each record being each class in the testing data
pred_prob_knn=predict(model_knn,X_test, type="prob")


###############################################################################
## Add preprocessing - but the embedded processing methods may for numeric vars only

#"corr" seeks to filter out highly correlated predictors
#Box-Cox (method = "BoxCox"), Yeo-Johnson (method = "YeoJohnson")
#exponential transformations (method = "expoTrans") 
#k-nearest neighbor imputation is carried out by finding the k closest samples (Euclidian distance) in the training set.
#Imputation via bagging fits a bagged tree model for each predictor (as a function of all the others). This method is simple, accurate and accepts missing values, but it has much higher computational cost.
#Imputation via medians takes the median of each predictor in the training set, and uses them to fill missing values. This method is simple, fast, and accepts missing values, but treats each predictor independently, and may be inaccurate.

#this standardized knn model has better accuracy than the knn one without standardization
model_knn_preproc_stand=train(income ~.,data = X_train, 
                              preProcess=c("center","scale"), #standardization
                              trControl=cv_5, method="knn" )

model_knn_prepro_medianimput=train(income~., data = X_train,
                                   preProcess="medianImpute", #median imput
                                   trControl=cv_5, method="knn")

#DID NOT WORK - MAYBE WE SHOULD CHANGE ? TO NA FOR IT TO WORK!!
model_knn_preproc_stand=train(income ~.,data = X_train, 
                              preProcess=c("center","scale"), #standardization
                              na.remove=TRUE, #should missing values be removed from                                                                 the calculation
                              trControl=cv_5, method="knn" )


########################################################################
#### Try more tuning parameters that the model considers
model_knn_tunepara=train(income ~.,data = X_train, 
                         preProcess=c("center","scale"), #standardization
                         tuneLength=7,
                         trControl=cv_5, method="knn" )


knn_tune=expand.grid(k=c(3,5,7,9,10))

model_knn_tunepara=train(income ~.,data = X_train, 
                         preProcess=c("center","scale"), #standardization
                         tuneGrid=knn_tune,
                         trControl=cv_5, method="knn" )

###### check whether it's a balanced dataset
table(X_train$income)


gbm_tune=expand.grid(interaction.depth=1:3,
                     n.trees=c(50,100,150),
                     shrinkage=0.1,
                     n.minobsinnode=10)

model_gbm_tunepara=train(income ~.,data = X_train, 
                         preProcess=c("center","scale"), #standardization
                         tuneGrid=gbm_tune,
                         trControl=cv_5, method="gbm" ) #gbm method has 4 parameters

##### plot model
plot(model_knn)
plot(model_knn_tunepara) #the accuracy is still increasing at k=16, maybe should try more k values
plot(model_gbm_tunepara)


#############################################################################
#### change evaluation metric from accuracy to sensitivity (or other metrics)
#twoClassSummary computes sensitivity, specificity and the area under the ROC curve.
cv_5_bina=trainControl(method = "cv", number = 5,
                       classProbs=TRUE, summaryFunction=twoClassSummary)

model_gbm_sens=train(income ~.,data = X_train, 
                     preProcess=c("center","scale"), #standardization
                     metric="Sens",
                     tuneGrid=gbm_tune,
                     trControl=cv_5_bina, 
                     method="gbm" ) #gbm

#WE'D BETTER THAN >50K & <50k TO SOMETHING LIKE HIGH/LOW TO AVOID ANY PROBLEMS
model_knn_sens=train(income ~.,data = X_train, 
                     preProcess=c("center","scale"), #standardization
                     metric="Sens",
                     tuneGrid=knn_tune,
                     trControl=cv_5_bina, 
                     method="knn" ) 


################################################################################
######### ONE S.E. RULE-what if a much simpler model can give slightly worse results (within 1 s.e. of sensitivity of the "best model")

model_knn_sens=train(income ~.,data = X_train, 
                     preProcess=c("center","scale"), #standardization
                     metric="Sens",
                     tuneGrid=knn_tune,
                     trControl=cv_5_bina, 
                     selectionFunction="oneSE", #the function used to select                                                    the optimal tuning parameters
                     method="knn" ) 

#make predictions
test_pred_knn=predict(model_knn_sens,X_test)


########################################################################
### create confusion matrix
conf_knn=confusionMatrix(data = test_pred_knn, reference=X_test$income)

#confusion matrix
conf_knn$table
#accuracy, sens etc
conf_knn$overall
conf_knn$byClass

#check which is set to be the POSITIVE CLASS - whether it's something we want
#if not, may need to change to:
conf_knn=confusionMatrix(data = test_pred_knn, reference=X_test$income,
                         postive="High") #or "Yes" or whatever the class is 


############################################################################
#### DEAL WITH IMBALANCED DATA SET
cv_5_down=trainControl(method = "cv", number = 5,
                       classProbs=TRUE, summaryFunction=twoClassSummary,
                       sampling = "down") #downsampling the majority class
#this is downsampling in the CROSS VALIDATION SET



```















